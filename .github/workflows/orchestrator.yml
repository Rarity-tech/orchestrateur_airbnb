name: Orchestrateur Airbnb Complet

on:
  workflow_dispatch: {}

jobs:
  orchestrate:
    runs-on: ubuntu-latest
    timeout-minutes: 420  # 7 heures max pour tout le workflow
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Python dependencies
        run: |
          python -m pip install -r requirements.txt
          python -m playwright install --with-deps chromium
          python -m pip install pandas

      - name: Install Node dependencies
        run: npm install

      - name: V√©rifier search_urls.txt
        run: |
          if [ ! -f search_urls.txt ]; then
            echo "‚ùå ERREUR: search_urls.txt est manquant!"
            echo "Cr√©ez ce fichier avec vos URLs de pages de recherche Airbnb."
            exit 1
          fi
          echo "‚úÖ search_urls.txt trouv√©"
          echo "Nombre de pages √† traiter: $(wc -l < search_urls.txt)"

      - name: Cr√©er les dossiers de sortie
        run: |
          mkdir -p output_phase1
          mkdir -p output_phase2
          mkdir -p output_final

      - name: Orchestration compl√®te (page par page)
        run: |
          echo "üöÄ D√âBUT DE L'ORCHESTRATION"
          echo "======================================"
          
          # Lire toutes les URLs
          mapfile -t SEARCH_URLS < search_urls.txt
          TOTAL_PAGES=${#SEARCH_URLS[@]}
          
          echo "üìä Nombre total de pages √† traiter: $TOTAL_PAGES"
          echo ""
          
          # Suivre les h√¥tes d√©j√† scrap√©s (gestion des doublons)
          touch hosts_already_scraped.txt
          
          PAGE_NUM=1
          for SEARCH_URL in "${SEARCH_URLS[@]}"; do
            # Nettoyer l'URL (enlever espaces, commentaires)
            SEARCH_URL=$(echo "$SEARCH_URL" | sed 's/#.*//' | xargs)
            
            # Ignorer les lignes vides
            if [ -z "$SEARCH_URL" ]; then
              continue
            fi
            
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            echo "üìÑ PAGE $PAGE_NUM/$TOTAL_PAGES"
            echo "üîó URL: $SEARCH_URL"
            echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
            
            # ==========================================
            # PHASE 1 : SCRAPER LES ANNONCES
            # ==========================================
            echo ""
            echo "1Ô∏è‚É£ PHASE 1 : Scraping des annonces..."
            
            export START_URL="$SEARCH_URL"
            export MAX_LISTINGS="20"
            export MAX_MINUTES="15"
            export PROXY=""
            
            # Ex√©cuter le scraper Python
            if python scrape_airbnb.py; then
              echo "‚úÖ Phase 1 r√©ussie pour page $PAGE_NUM"
              
              # Sauvegarder le r√©sultat
              if [ -f airbnb_results.csv ]; then
                cp airbnb_results.csv "output_phase1/page_${PAGE_NUM}_listings.csv"
                
                # Extraire les URLs des h√¥tes (CORRECTION: utiliser Python pour parser CSV correctement)
                echo "üìã Extraction des URLs des h√¥tes..."
                python3 << 'EXTRACT_URLS' > temp_hosts_page_${PAGE_NUM}.txt
import csv
import sys

try:
    with open('airbnb_results.csv', 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            url = row.get('host_profile_url', '').strip()
            if url and url.startswith('https://'):
                print(url)
except Exception as e:
    print(f"Erreur extraction: {e}", file=sys.stderr)
EXTRACT_URLS
                
                # Debug: afficher le nombre d'URLs trouv√©es
                URL_COUNT=$(wc -l < temp_hosts_page_${PAGE_NUM}.txt 2>/dev/null || echo 0)
                echo "üìä URLs extraites: $URL_COUNT"
                
                # Filtrer les h√¥tes d√©j√† scrap√©s
                if [ -s temp_hosts_page_${PAGE_NUM}.txt ]; then
                  sort temp_hosts_page_${PAGE_NUM}.txt | uniq > temp_hosts_unique.txt
                  comm -23 temp_hosts_unique.txt <(sort hosts_already_scraped.txt) > hosts_to_scrape_page_${PAGE_NUM}.txt
                  
                  HOST_COUNT=$(wc -l < hosts_to_scrape_page_${PAGE_NUM}.txt)
                  echo "üìä H√¥tes uniques √† scraper: $HOST_COUNT"
                  
                  if [ "$HOST_COUNT" -gt 0 ]; then
                    # ==========================================
                    # PHASE 2 : SCRAPER LES PROFILS H√îTES
                    # ==========================================
                    echo ""
                    echo "2Ô∏è‚É£ PHASE 2 : Scraping des profils h√¥tes ($HOST_COUNT h√¥tes)..."
                    
                    # Pr√©parer urls.txt pour le scraper Node.js
                    cp hosts_to_scrape_page_${PAGE_NUM}.txt urls.txt
                    
                    # Debug: afficher les premi√®res URLs
                    echo "üìù Premi√®res URLs √† scraper:"
                    head -3 urls.txt || true
                    
                    # Ex√©cuter le scraper Node.js
                    if npm start; then
                      echo "‚úÖ Phase 2 r√©ussie pour page $PAGE_NUM"
                      
                      # Sauvegarder le r√©sultat
                      if [ -f output/results.csv ]; then
                        cp output/results.csv "output_phase2/page_${PAGE_NUM}_hosts.csv"
                        
                        # Ajouter les h√¥tes scrap√©s √† la liste
                        cat hosts_to_scrape_page_${PAGE_NUM}.txt >> hosts_already_scraped.txt
                        sort -u hosts_already_scraped.txt -o hosts_already_scraped.txt
                      else
                        echo "‚ö†Ô∏è Aucun r√©sultat de Phase 2 pour page $PAGE_NUM"
                      fi
                    else
                      echo "‚ùå ERREUR Phase 2 pour page $PAGE_NUM - CONTINUATION"
                    fi
                  else
                    echo "‚ÑπÔ∏è Tous les h√¥tes de cette page ont d√©j√† √©t√© scrap√©s"
                  fi
                else
                  echo "‚ö†Ô∏è Aucune URL d'h√¥te extraite de la page $PAGE_NUM"
                  echo "üîç Debug: V√©rification du CSV..."
                  head -2 airbnb_results.csv || true
                fi
              else
                echo "‚ö†Ô∏è Aucun r√©sultat de Phase 1 pour page $PAGE_NUM"
              fi
            else
              echo "‚ùå ERREUR Phase 1 pour page $PAGE_NUM - CONTINUATION"
            fi
            
            echo ""
            echo "‚úÖ Page $PAGE_NUM/$TOTAL_PAGES termin√©e"
            echo ""
            
            # Petit d√©lai entre les pages pour √©viter le rate limiting
            if [ $PAGE_NUM -lt $TOTAL_PAGES ]; then
              echo "‚è≥ Attente de 10 secondes avant la page suivante..."
              sleep 10
            fi
            
            PAGE_NUM=$((PAGE_NUM + 1))
          done
          
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "üéâ ORCHESTRATION TERMIN√âE"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

      - name: Fusion des r√©sultats
        run: |
          echo "üîó FUSION DES R√âSULTATS..."
          python merge_results.py

      - name: Afficher le r√©sum√©
        run: |
          echo ""
          echo "üìä R√âSUM√â DES R√âSULTATS"
          echo "======================================"
          
          if [ -d output_phase1 ]; then
            PHASE1_COUNT=$(ls -1 output_phase1/*.csv 2>/dev/null | wc -l)
            echo "Phase 1 (annonces): $PHASE1_COUNT fichiers"
          fi
          
          if [ -d output_phase2 ]; then
            PHASE2_COUNT=$(ls -1 output_phase2/*.csv 2>/dev/null | wc -l)
            echo "Phase 2 (h√¥tes): $PHASE2_COUNT fichiers"
          fi
          
          if [ -f output_final/final_complete_results.csv ]; then
            TOTAL_ROWS=$(tail -n +2 output_final/final_complete_results.csv | wc -l)
            echo "R√©sultat final: $TOTAL_ROWS lignes de donn√©es"
            echo "‚úÖ Fichier: output_final/final_complete_results.csv"
          else
            echo "‚ö†Ô∏è Aucun fichier final cr√©√©"
          fi

      - name: Upload r√©sultats Phase 1
        uses: actions/upload-artifact@v4
        with:
          name: phase1-listings
          path: output_phase1/*.csv
          if-no-files-found: warn

      - name: Upload r√©sultats Phase 2
        uses: actions/upload-artifact@v4
        with:
          name: phase2-hosts
          path: output_phase2/*.csv
          if-no-files-found: warn

      - name: Upload r√©sultat final fusionn√©
        uses: actions/upload-artifact@v4
        with:
          name: final-complete-results
          path: output_final/final_complete_results.csv
          if-no-files-found: error

      - name: Upload debug info
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: debug-info
          path: |
            hosts_already_scraped.txt
            temp_hosts_page_*.txt
            hosts_to_scrape_page_*.txt
            output/debug/**
          if-no-files-found: ignore
